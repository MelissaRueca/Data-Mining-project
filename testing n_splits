import os
import pandas as pd
import re
import string
import pickle
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Download required NLTK data
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('punkt', quiet=True)
nltk.download('punkt_tab', quiet=True) 

# Load reviews
def load_reviews(path, label):
    """
    Load reviews from fold directories and assign labels.
    """
    data = []
    for fold in range(1,6):
        folder = path + f"/fold{fold}"
        for file in os.listdir(folder):
            with open(os.path.join(folder, file), "r", encoding="utf-8") as f:
                text = f.read().strip()
                data.append({
                    "text": text,
                    "label": label,
                    "fold": fold
                })
    return pd.DataFrame(data)

# Load and merge datasets 
truthful_df = load_reviews("negative_polarity/truthful_from_Web", "truthful") 
deceptive_df = load_reviews("negative_polarity/deceptive_from_MTurk", "deceptive") 
df = pd.concat([truthful_df, deceptive_df], ignore_index=True) 

# Keep negations
stop_words = set(stopwords.words('english')) 
stop_words.discard('not') 
stop_words.discard('no')
stop_words.discard("n't") 

lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    """
    Preprocessing Text :
    1. Convert to Lowercase
    2. Removes HTML tags
    3. Removes punctuation and digits
    4. Tokenization
    5. Stopword removal (keeping negations)
    6. Lemmatization
    """
    text = text.lower()
    text = re.sub(r'<.*?>', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation + string.digits))
    tokens = nltk.word_tokenize(text)
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

# Apply preprocessing 
df["clean_text"] = df["text"].apply(preprocess_text)

# Split folds 1-4 for training and fold 5 for testing
train_df = df[df["fold"] != 5]
test_df = df[df["fold"] == 5]

# Extract labels
y_train = train_df["label"]
y_test = test_df["label"]

# Feature extraction

# Unigrams (single words)
tfidf_uni = TfidfVectorizer(
        max_df=0.95,  # exclude terms that appear in >95% of documents
        min_df=2, # exclude terms that appear <2 documents
        ngram_range=(1,1), 
        lowercase=False  
)

X_train_uni = tfidf_uni.fit_transform(train_df["clean_text"])
X_test_uni= tfidf_uni.transform(test_df["clean_text"]) 

# Unigrams + bigrams (single + two-word phrases)
tfidf_bi = TfidfVectorizer(
        max_df=0.95,
        min_df=2,
        ngram_range=(1, 2),
        lowercase=False
)

X_train_bi = tfidf_bi.fit_transform(train_df["clean_text"])
X_test_bi = tfidf_bi.transform(test_df["clean_text"])

# Save processed data
df.to_csv("processed_reviews.csv", index=False) 
train_df.to_csv("train_reviews.csv", index=False)
test_df.to_csv("test_reviews.csv", index=False)

# Save vectorizers 
with open("tfidf_uni.pkl", "wb") as f:
    pickle.dump(tfidf_uni, f)
with open("tfidf_bi.pkl", "wb") as f:
    pickle.dump(tfidf_bi, f)

print("Data preprocessing complete.")


import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegressionCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ----------------------------
# Train/validate on folds 1–4; test on fold 5 (you already created train_df/test_df)
# ----------------------------

def run_lr_l1_cv(train_text, train_y, test_text, test_y, ngram_range=(1,1), name="Unigrams"):
    print(f"\n=== {name} (ngram_range={ngram_range}) ===")

    # Build pipeline: TF-IDF -> LogisticRegressionCV with L1 penalty
    pipe = Pipeline([
        ("tfidf", TfidfVectorizer(
            max_df=0.95,
            min_df=2,
            ngram_range=ngram_range,
            lowercase=False
        )),
        ("lr", LogisticRegressionCV(
            # Cs are 1/λ; search on a log grid
            Cs=np.logspace(-3, 1, 5),    # 0.001 ... 1000
            cv=StratifiedKFold(n_splits=4, shuffle=True, random_state=42),
            penalty="l1",
            solver="liblinear",           # supports L1 well for binary problems
            scoring="accuracy",
            max_iter=5000,
            n_jobs=-1,
            refit=True,
            random_state=42
        ))
    ])

    # Fit on training folds (1–4) with internal CV to choose C
    pipe.fit(train_text, train_y)

    # Report chosen C (per class; for binary problems these are usually equal)
    best_C = pipe.named_steps["lr"].C_
    print("Selected C (per class):", best_C)

    # Evaluate on held-out fold 5
    y_pred = pipe.predict(test_text)
    acc = accuracy_score(test_y, y_pred)
    print("Test Accuracy:", f"{acc:.4f}")
    print("\nClassification Report:\n", classification_report(test_y, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(test_y, y_pred))

    return acc, best_C, pipe

# Run for unigrams
acc_uni, C_uni, model_uni = run_lr_l1_cv(
    train_df["clean_text"], y_train,
    test_df["clean_text"], y_test,
    ngram_range=(1,1),
    name="Unigram TF-IDF"
)

# Run for unigrams + bigrams
acc_bi, C_bi, model_bi = run_lr_l1_cv(
    train_df["clean_text"], y_train,
    test_df["clean_text"], y_test,
    ngram_range=(1,2),
    name="Unigram+Bigram TF-IDF"
)

print(f"\n=== Final Comparison ===\nUnigrams  : {acc_uni:.4f}\nUni+Bigrams: {acc_bi:.4f}")

import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegressionCV
from sklearn.metrics import accuracy_score

split_values = [3, 4, 5, 8, 10]
results = []

for n in split_values:
    print(f"\n=== Running CV with n_splits = {n} ===")
    
    pipe = Pipeline([
        ("tfidf", TfidfVectorizer(
            max_df=0.95,
            min_df=2,
            ngram_range=(1, 2),   # you can change to (1,1) for unigrams
            lowercase=False
        )),
        ("lr", LogisticRegressionCV(
             Cs=np.logspace(-3, 1, 5), 
            cv=StratifiedKFold(n_splits=n, shuffle=True, random_state=42),
            penalty="l1",
            solver="liblinear",
            scoring="accuracy",
            max_iter=5000,
            n_jobs=-1,
            random_state=42
        ))
    ])

    # Fit on folds 1–4 (train_df)
    pipe.fit(train_df["clean_text"], y_train)

    # Evaluate on held-out fold 5
    y_pred = pipe.predict(test_df["clean_text"])
    acc = accuracy_score(y_test, y_pred)
    best_C = pipe.named_steps["lr"].C_[0]

    print(f"Best C: {best_C:.4f} | Test Accuracy: {acc:.4f}")
    results.append((n, best_C, acc))

# Summarize
print("\n=== Summary across n_splits ===")
for n, C, acc in results:
    print(f"n_splits={n:<2} → Best C={C:<8.4f} | Test Acc={acc:.4f}")

